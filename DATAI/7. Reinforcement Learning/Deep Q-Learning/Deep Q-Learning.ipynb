{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb310237",
   "metadata": {},
   "source": [
    "# Q-Learning vs Deep Q-learning\n",
    "\n",
    "* Q-Learning yeni bir ortama girdiğinde ve daha önce karşılaşmadığı bir durumla karşılaştığında (Q-table'da bulunmayan bir durum) rastgele bir eylem yapar ve bu istemeyen bir durumdur.\n",
    "\n",
    "* Durumlar ve eylemler ne kadar çoksa Q-table o kadar büyük olur. Bu durum kaynak ve performansta eksiklik yaşanmasına sebep olur.\n",
    "\n",
    "* Bu durumları çözmek için Deep Q-Learning algoritması kullanılır.\n",
    "\n",
    "State + Action --> Q-Table --> Q-value\n",
    "\n",
    "State --> Deep Q-Neural Network --> Q-value action1, Q-value action2\n",
    "\n",
    "![Q-learning vs Deep Q-learning](1-s2.0-S0968090X18318862-gr5.jpg)\n",
    "\n",
    "## DQL Pseudo Code\n",
    "1. Initialize replay memory capacity.\n",
    "2. Initialize the network with random weights.\n",
    "3. _For each episode:_\n",
    "    1. Initialize the starting state.\n",
    "    2. _For each time step:_\n",
    "        1. Select an action.\n",
    "            * _Via exploration or exploitation._\n",
    "        2. Execute selected action in an emulator.\n",
    "        3. Observe reward and next state.\n",
    "        4. Store experience in replay memory.\n",
    "        \n",
    "        \n",
    "1. Initialize replay memory $D$ to capacity $N$\n",
    "2. Initialize actio-value function $Q$ with random weights\n",
    "3. $\\text{for }$ episode $= 1, M \\text{ do}$ \n",
    "    1. Initialize sequence $s_1 = \\{x_1\\}$ and preprocessed sequenced $ \\phi = \\phi(s_1)$\n",
    "    2. $\\text{for }t = 1, T \\text{ do}$\n",
    "        1. With probability $\\epsilon$ select a random action $a_t$\n",
    "        2. otherwise select $ a_t = max_aQ*(\\phi(s_t),a;\\theta)$\n",
    "        3. Execute action a_t in emulator and observe reward r_t and image $x_{t+1}$\n",
    "        4. Set $s_{t+1} = s_t,a_t,x_{t+1}$ and preprocess $\\phi_{t+1} = \\phi(s_{t+1})$\n",
    "        5. Store transition $(\\phi_t,a_t,r_t,Q_{t+1})$ in $D$\n",
    "        6. Saple random minibatch of transitions $(\\phi_t,a_t,r_t,Q_{t+1})$ from $D$\n",
    "        7. Set $ y_j = \\begin{cases}\n",
    "r_j& \\text{for terminal} \\phi_{j+1}, \\\\\n",
    "r_j+\\gamma max_{a'}Q(\\phi_{j+1} a^{'};\\theta)),& \\text{for non-terminal} \\phi_{j+1}.\n",
    "\\end{cases} $\n",
    "        8. Perform a gradient descent step on $(y_j - Q(\\phi_j,a_j;\\theta))^2$ according to equation 3\n",
    "    3. $\\text{end for}$\n",
    "4. $\\text{end for}$\n",
    "        \n",
    "        \n",
    "## Experience Replay\n",
    "\n",
    "Agent'ın kazandığı deneyimleri unutmamasını sağlamak için kullanılır. Adımlar arasındaki korelasyonu azatmak amaçlanır.\n",
    "\n",
    "### Exploitation vs Exploration\n",
    "\n",
    "$ \\epsilon - \\text{Greedy} $\n",
    "\n",
    "$ a = \\begin{cases}\n",
    "a& 1 - \\epsilon \\\\\n",
    "\\text{random action}& \\epsilon\n",
    "\\end{cases} $\n",
    "\n",
    "$ \\epsilon = \\text{Probability of Exploration} $\n",
    "\n",
    "```python\n",
    "def adaptiveEGreedy(self):\n",
    "    if self.epsilon > self.epsilon_min:\n",
    "        self.epsilon *= self.epsilon_decay\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5177052",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
